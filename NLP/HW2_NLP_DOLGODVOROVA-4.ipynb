{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2 NLP DOLGODVOROVA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLQgPdNjAgdN"
      },
      "source": [
        "**ЧАСТЬ 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrQg4wI9AlkA"
      },
      "source": [
        "*Шаш 1 - импорт всего необходимого*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFhY_qgiG6Ys"
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "    Doc\n",
        ")\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "m = MorphAnalyzer()\n",
        "from pymystem3 import Mystem\n",
        "mystem = Mystem()\n",
        "import spacy\n",
        "import flair\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ5PlhJEAqyg"
      },
      "source": [
        "*Шаг 2 - разметка текстов для русского и английского*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybpw2TqYHCUa"
      },
      "source": [
        "russian_text = ['До меня доносится легкий запах духов',\n",
        "'Русичка запретила болтать на уроках',\n",
        "'Была бонбандировка города',\n",
        "'Сегодня у нас была ген. уборка',\n",
        "'Махабхарата ужасный сериал',\n",
        "'Печь печенья всегда весело',\n",
        "'Многочасовой рейс из Москвы в Петербург',\n",
        "'Я наелась до отвала',\n",
        "'Я смеялась без удержу',\n",
        "'Я учусь в вышке',\n",
        "'Надеюсь она приедет',\n",
        "'Она мне сказала шо мазик вредный для здоровья',\n",
        "'Вчера изучала со студентами свежий проект закона о Госсовете',\n",
        "'С пылу с жару только из печки пирожок', \n",
        "'Подробности и перспективы пустоты разъяснила народной газете Московский комсомолец',\n",
        "'Ща буду есть бутир с тофу и мазиком',\n",
        "'Выход на сушу есть величайшая геополитическая катастрофа нашего миллиардолетия',\n",
        "'Твиттер моя самоя любимая соцсеть',\n",
        "'Хотя фейсбук тоже ничего',\n",
        "'А вконтакте полный ужас']\n",
        "\n",
        "russian_tag = ['PREP PRON VERB ADJ NOUN NOUN',\n",
        "                'NOUN VERB VERB PREP NOUN',\n",
        "                'VERB NOUN NOUN',\n",
        "                'ADV PREP PRON VERB ADJ NOUN',\n",
        "                'NOUN ADJ NOUN',\n",
        "                'VERB NOUN ADV ADV',\n",
        "                'ADJ NOUN PREP NOUN PREP NOUN',\n",
        "                'PRON VERB PREP NOUN',\n",
        "                'PRON VERB PREP NOUN',\n",
        "                'PRON VERB PREP NOUN',\n",
        "               'VERB PRON VERB',\n",
        "               'PRON PRON VERB CONJ NOUN ADJ PREP NOUN',\n",
        "                'ADV VERB PREP NOUN ADJ NOUN NOUN PREP NOUN',\n",
        "                'PREP NOUN PREP NOUN ADV PREP NOUN NOUN',\n",
        "                'NOUN CONJ NOUN NOUN VERB ADJ NOUN ADJ NOUN',\n",
        "                'ADV VERB VERB NOUN PREP NOUN CONJ NOUN',\n",
        "                'NOUN PREP NOUN VERB ADJ ADJ NOUN PRON NOUN',\n",
        "                'NOUN PRON ADJ ADJ NOUN',\n",
        "                'CONJ NOUN ADV NOUN',\n",
        "                'CONJ NOUN ADJ NOUN']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLqhrsQcHHIf"
      },
      "source": [
        "english_text = ['i goed to the shop',\n",
        "              'twitter is my absolutely fav social media',\n",
        "              'facebook is the way to baaad',\n",
        "              'I totally love your twts',\n",
        "              'This is a subtweet of anyone in particular and it is going to annoy some people',\n",
        "             'I have a lot of PhD followers and friends and I am a stubborn guy myself',\n",
        "             'but one thing I am noticed is when someone gets a PhD it become 10 harder convince them they are wrong',\n",
        "              'I have actually found the opposite in my discipline',\n",
        "             'The PhD are generally open to critique and questioning of their work',\n",
        "              'It is the people who have an MBA but no practical business experience that are absolute monsters'\n",
        "\n",
        "              ]\n",
        "\n",
        "english_tag = ['PRON VERB PART ART NOUN',\n",
        "               'NOUN VERB PRON ADV ADJ ADJ NOUN',\n",
        "               'NOUN VERB ART NOUN PART ADJ',\n",
        "               'PRON ADV VERB PRON NOUN',\n",
        "               'DET VERB ART NOUN PREP PRON PREP ADJ CONJ PRON VERB VERB PART VERB ADJ NOUN',\n",
        "               'PRON VERB ART ADV PREP NOUN NOUN CONJ NOUN CONJ PRON VERB ART ADJ NOUN PRON',\n",
        "               'CONJ NUM NOUN PRON VERB VERB VERB ADV PRON VERB ART NOUN PRON VERB NUM ADJ VERB PRON PRON VERB ADJ',\n",
        "               'PRON VERB ADV VERB ART ADJ PREP PRON NOUN',\n",
        "               'ART NOUN VERB ADV ADJ PART NOUN CONJ NOUN PREP PRON NOUN',\n",
        "               'PRON VERB ART NOUN ADV VERB ART NOUN CONJ PART ADJ ADJ NOUN DET VERB ADJ NOUN'\n",
        "               ]\n",
        "               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhznkFgVIXlS"
      },
      "source": [
        "**Почему я выбрала именно эти тексты?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc_FfjrvIa0p"
      },
      "source": [
        "В русском много словоформ, которые могут быть неизвестны парсеру - шо, мазик и тд. Также есть сокращения - ген. уборка, которые часто определяются неверно (сужу по опыту с TurkuNLP). Также есть опечатки типа \"бонбардировка\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPfRrOT8Iv1g"
      },
      "source": [
        "В английском также есть вероятно неизвестные парсеру словоформы (например, неправильное образование прошедшего времени от глагола do - doed). Также есть много сокращений типа PHD, которые, как правило, тоже не очень хорошо распознаются парсерами "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUzedCH_CkqK"
      },
      "source": [
        "*Шаг 3 - очистка текста*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzvR84xVHr0b"
      },
      "source": [
        "def clean_my_texts(texts, tags):\n",
        "  clean_texts = []\n",
        "  new_tags = []\n",
        "  for i in texts:\n",
        "    i = i.lower()\n",
        "    for znak in punctuation:\n",
        "      if znak in i:\n",
        "        i = i.replace(znak, '')\n",
        "        i = i.replace('  ', ' ')\n",
        "    i = i.split(' ')\n",
        "    for n in i:\n",
        "      clean_texts.append(n)\n",
        "    new_tags = []\n",
        "  for tag in tags:\n",
        "    tag = tag.split(' ')\n",
        "    for t in tag:\n",
        "      new_tags.append(t)\n",
        "  return clean_texts, new_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeT9GrYFHwME",
        "outputId": "73adb567-2a7e-4898-8a9f-d64d48efcf31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "russian_clean = clean_my_texts(russian_text, russian_tag)\n",
        "print(russian_clean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['до', 'меня', 'доносится', 'легкий', 'запах', 'духов', 'русичка', 'запретила', 'болтать', 'на', 'уроках', 'была', 'бонбандировка', 'города', 'сегодня', 'у', 'нас', 'была', 'ген', 'уборка', 'махабхарата', 'ужасный', 'сериал', 'печь', 'печенья', 'всегда', 'весело', 'многочасовой', 'рейс', 'из', 'москвы', 'в', 'петербург', 'я', 'наелась', 'до', 'отвала', 'я', 'смеялась', 'без', 'удержу', 'я', 'учусь', 'в', 'вышке', 'надеюсь', 'она', 'приедет', 'она', 'мне', 'сказала', 'шо', 'мазик', 'вредный', 'для', 'здоровья', 'вчера', 'изучала', 'со', 'студентами', 'свежий', 'проект', 'закона', 'о', 'госсовете', 'с', 'пылу', 'с', 'жару', 'только', 'из', 'печки', 'пирожок', 'подробности', 'и', 'перспективы', 'пустоты', 'разъяснила', 'народной', 'газете', 'московский', 'комсомолец', 'ща', 'буду', 'есть', 'бутир', 'с', 'тофу', 'и', 'мазиком', 'выход', 'на', 'сушу', 'есть', 'величайшая', 'геополитическая', 'катастрофа', 'нашего', 'миллиардолетия', 'твиттер', 'моя', 'самоя', 'любимая', 'соцсеть', 'хотя', 'фейсбук', 'тоже', 'ничего', 'а', 'вконтакте', 'полный', 'ужас'], ['PREP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'PREP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'ADV', 'ADV', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'VERB', 'PRON', 'VERB', 'PRON', 'PRON', 'VERB', 'CONJ', 'NOUN', 'ADJ', 'PREP', 'NOUN', 'ADV', 'VERB', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'ADV', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'NOUN', 'NOUN', 'PRON', 'ADJ', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'ADV', 'NOUN', 'CONJ', 'NOUN', 'ADJ', 'NOUN'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdSF1WdRHxZh",
        "outputId": "d0cbf923-4bab-40e6-b3b5-24143f8883af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "english_clean = clean_my_texts(english_text, english_tag)\n",
        "print(english_clean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['i', 'goed', 'to', 'the', 'shop', 'twitter', 'is', 'my', 'absolutely', 'fav', 'social', 'media', 'facebook', 'is', 'the', 'way', 'to', 'baaad', 'i', 'totally', 'love', 'your', 'twts', 'this', 'is', 'a', 'subtweet', 'of', 'anyone', 'in', 'particular', 'and', 'it', 'is', 'going', 'to', 'annoy', 'some', 'people', 'i', 'have', 'a', 'lot', 'of', 'phd', 'followers', 'and', 'friends', 'and', 'i', 'am', 'a', 'stubborn', 'guy', 'myself', 'but', 'one', 'thing', 'i', 'am', 'noticed', 'is', 'when', 'someone', 'gets', 'a', 'phd', 'it', 'become', '10', 'harder', 'convince', 'them', 'they', 'are', 'wrong', 'i', 'have', 'actually', 'found', 'the', 'opposite', 'in', 'my', 'discipline', 'the', 'phd', 'are', 'generally', 'open', 'to', 'critique', 'and', 'questioning', 'of', 'their', 'work', 'it', 'is', 'the', 'people', 'who', 'have', 'an', 'mba', 'but', 'no', 'practical', 'business', 'experience', 'that', 'are', 'absolute', 'monsters'], ['PRON', 'VERB', 'PART', 'ART', 'NOUN', 'NOUN', 'VERB', 'PRON', 'ADV', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ART', 'NOUN', 'PART', 'ADJ', 'PRON', 'ADV', 'VERB', 'PRON', 'NOUN', 'DET', 'VERB', 'ART', 'NOUN', 'PREP', 'PRON', 'PREP', 'ADJ', 'CONJ', 'PRON', 'VERB', 'VERB', 'PART', 'VERB', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ART', 'ADV', 'PREP', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'PRON', 'VERB', 'ART', 'ADJ', 'NOUN', 'PRON', 'CONJ', 'NUM', 'NOUN', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV', 'PRON', 'VERB', 'ART', 'NOUN', 'PRON', 'VERB', 'NUM', 'ADJ', 'VERB', 'PRON', 'PRON', 'VERB', 'ADJ', 'PRON', 'VERB', 'ADV', 'VERB', 'ART', 'ADJ', 'PREP', 'PRON', 'NOUN', 'ART', 'NOUN', 'VERB', 'ADV', 'ADJ', 'PART', 'NOUN', 'CONJ', 'NOUN', 'PREP', 'PRON', 'NOUN', 'PRON', 'VERB', 'ART', 'NOUN', 'ADV', 'VERB', 'ART', 'NOUN', 'CONJ', 'PART', 'ADJ', 'ADJ', 'NOUN', 'DET', 'VERB', 'ADJ', 'NOUN'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkQtwBmLA1gx"
      },
      "source": [
        "**ЧАСТЬ 2 - тестируем все парсеры**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUo5AW5m8FMS"
      },
      "source": [
        "**PYMORPHY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMPOTm-fH0j9",
        "outputId": "e51d17fc-4f9e-4ebd-c474-38dd3ddd200a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pymorphy_tags = []\n",
        "pymorphy_convert = []\n",
        "m = MorphAnalyzer()\n",
        "\n",
        "for i in russian_clean[0]:\n",
        "  parse = m.parse(i)[0].tag.POS\n",
        "  pymorphy_tags.append(parse)\n",
        "\n",
        "for tag in pymorphy_tags:  # меняем теги на нужные нам\n",
        "\n",
        "  tag = re.sub('NUMR$', 'NUM', tag)\n",
        "  tag = re.sub('ADVB$', 'ADV', tag)\n",
        "  tag = re.sub('COMP$', 'ADJ', tag)\n",
        "  tag = re.sub('GRND$', 'VERB', tag)\n",
        "  tag = re.sub('PRCL$', 'PART', tag)\n",
        "  tag = re.sub('ADJF$', 'ADJ', tag)\n",
        "  tag = re.sub('ADJS$', 'ADJ', tag)\n",
        "  tag = re.sub('INFN$', 'VERB', tag)\n",
        "  tag = re.sub('PRTF$', 'VERB', tag)\n",
        "  tag = re.sub('PRED$', 'ADV', tag)\n",
        "  tag = re.sub('PRTS$', 'VERB', tag)\n",
        "  tag = re.sub('NPRO$', 'PRON', tag)\n",
        "  \n",
        "  pymorphy_convert.append(tag)\n",
        "\n",
        "print(pymorphy_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PREP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'PREP', 'PRON', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'ADV', 'ADV', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'VERB', 'PRON', 'VERB', 'PREP', 'NOUN', 'VERB', 'PRON', 'VERB', 'PRON', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADJ', 'PREP', 'NOUN', 'ADV', 'VERB', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'ADV', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'PART', 'ADV', 'CONJ', 'NOUN', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLeUlaj4H6gu",
        "outputId": "bac6a09a-9d55-48bb-bbe1-12dca6257072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(russian_clean[1], pymorphy_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9196428571428571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6gCBwFQ8Xgh"
      },
      "source": [
        "**NATASHA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8iZkKix8YFI"
      },
      "source": [
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Alxm_4v9bi6",
        "outputId": "e0705bbd-4c43-4793-daf5-0c84bf69c6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "natasha = []\n",
        "n = ''\n",
        "for i in russian_clean[0]:\n",
        "  \n",
        "  n += i + ' '\n",
        "doc = Doc(n)\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "\n",
        "for d in doc.tokens:\n",
        "\n",
        "  natasha.append(d.pos)\n",
        "natasha_convert = []\n",
        "\n",
        "for nat in natasha:  # меняем теги на нужные нам\n",
        "\n",
        "  nat = re.sub('AUX$', 'VERB', nat)\n",
        "  nat = re.sub('ADP$', 'PREP', nat)\n",
        "  nat = re.sub('CCONJ$', 'CONJ', nat)\n",
        "  nat = re.sub('SCONJ$', 'CONJ', nat)\n",
        "  natasha_convert.append(nat)\n",
        "\n",
        "print(natasha_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PREP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'NOUN', 'VERB', 'VERB', 'NOUN', 'ADV', 'PREP', 'PRON', 'VERB', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'ADV', 'ADV', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'PRON', 'VERB', 'PRON', 'PRON', 'VERB', 'PART', 'ADV', 'ADJ', 'PREP', 'NOUN', 'ADV', 'VERB', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PART', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'DET', 'NOUN', 'NOUN', 'DET', 'ADJ', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'ADV', 'PRON', 'CONJ', 'ADV', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btrGLBIN-y9b",
        "outputId": "aa5d59f8-be59-4d8b-de86-a5d9394788e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(russian_clean[1], natasha_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LsVBdocCsVe"
      },
      "source": [
        "**MYSTEM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXNY0RE9Jl4E",
        "outputId": "e8b61709-fede-440b-ab2d-124bab64efd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz  # иначе pipe крашится и вообще что-то страшное происходит\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 15:52:30--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.245, 5.45.205.241, 5.45.205.244, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.245|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskmar13.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2020-10-18 15:52:30--  http://cache-mskmar13.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-mskmar13.cdn.yandex.net (cache-mskmar13.cdn.yandex.net)... 5.45.222.86, 2a02:6b8:0:2b03::86\n",
            "Connecting to cache-mskmar13.cdn.yandex.net (cache-mskmar13.cdn.yandex.net)|5.45.222.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz.1’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  9.37MB/s    in 1.7s    \n",
            "\n",
            "2020-10-18 15:52:33 (9.37 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz.1’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlBfaq5UJdg1",
        "outputId": "4c635e6e-77b1-4686-dae7-62ee306eb3e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "mystem_tags = []\n",
        "mystem_convert = []\n",
        "\n",
        "for i in russian_clean[0]:\n",
        "  m = mystem.analyze(i)\n",
        "  try:\n",
        "    p = (m[0]['analysis'][0]['gr']).split('=')[0].split(',')[0]\n",
        "  except:\n",
        "    IndexError\n",
        "  mystem_tags.append(p)\n",
        "\n",
        "for tag in mystem_tags:  # меняем теги на нужные нам\n",
        "\n",
        "  tag = re.sub('A$', 'ADJ', tag)\n",
        "  tag = re.sub('SPRO$', 'PRON', tag)\n",
        "  tag = re.sub('PR$', 'PREP', tag)\n",
        "  tag = re.sub('ADVPRO$', 'ADV', tag)\n",
        "  tag = re.sub('ANUM$', 'NUM', tag)\n",
        "  tag = re.sub('V$', 'VERB', tag)\n",
        "  tag = re.sub('APRO$', 'PRON', tag)\n",
        "  tag = re.sub('S$', 'NOUN', tag)\n",
        "  mystem_convert.append(tag)\n",
        "\n",
        "print(mystem_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PREP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADVERB', 'PREP', 'PRON', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'ADVERB', 'ADVERB', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PRON', 'VERB', 'PREP', 'VERB', 'PRON', 'VERB', 'NOUN', 'NOUN', 'VERB', 'PRON', 'VERB', 'PRON', 'PRON', 'VERB', 'VERB', 'NOUN', 'ADJ', 'PREP', 'NOUN', 'ADVERB', 'VERB', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'INTJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PART', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'VERB', 'ADVERB', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'NOUN', 'NOUN', 'PRON', 'ADVERB', 'ADJ', 'NOUN', 'PART', 'NOUN', 'PART', 'ADVERB', 'PART', 'NOUN', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvOBntPhmgh_",
        "outputId": "65e8c546-978d-4c32-8d0a-b7fc3f914ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(russian_clean[1], mystem_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8035714285714286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUaqYgjJV4S"
      },
      "source": [
        "**SPACY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8TlEMtprI8D",
        "outputId": "b750c799-ac94-4cfa-de57-5c9370ae6e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tags_spacy = []\n",
        "spacy_convert = []\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "s = ''\n",
        "for word in english_clean[0]:  # конвертируем в строку \n",
        "  s = s + word + ' '\n",
        "doc = nlp(s)\n",
        "\n",
        "for token in doc:\n",
        "    tags_spacy.append(token.pos_)\n",
        "\n",
        "for tag in tags_spacy:  # заменяем теги на нужные нам\n",
        "\n",
        "  tag = re.sub('PROPN$', 'NOUN', tag)\n",
        "  tag = re.sub('ADP$', 'PREP', tag)\n",
        "  tag = re.sub('CCONJ$', 'CONJ', tag)\n",
        "  tag = re.sub('AUX$', 'VERB', tag)\n",
        "  tag = re.sub('SCONJ$', 'CONJ', tag)\n",
        "\n",
        "  spacy_convert.append(tag)\n",
        "  \n",
        "print(spacy_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PRON', 'VERB', 'PREP', 'DET', 'NOUN', 'NOUN', 'VERB', 'DET', 'ADV', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'PRON', 'ADV', 'VERB', 'DET', 'NOUN', 'DET', 'VERB', 'DET', 'NOUN', 'PREP', 'PRON', 'PREP', 'ADJ', 'CONJ', 'PRON', 'VERB', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'CONJ', 'NUM', 'NOUN', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV', 'PRON', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'NUM', 'ADJ', 'PREP', 'PRON', 'PRON', 'VERB', 'ADJ', 'PRON', 'VERB', 'ADV', 'VERB', 'DET', 'ADJ', 'PREP', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'ADV', 'ADJ', 'PREP', 'NOUN', 'CONJ', 'VERB', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'CONJ', 'DET', 'ADJ', 'NOUN', 'NOUN', 'DET', 'VERB', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzyLs64bLTw7",
        "outputId": "32cf689e-703f-4327-b1f9-7a62dbd9bc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(english_clean[1], spacy_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7807017543859649"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is-wpZ2lxRyH"
      },
      "source": [
        "**NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMtAT7i0yOsW",
        "outputId": "cb849515-7693-4cb0-e9c2-53caceb0dd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du3Lf1tSxT6g",
        "outputId": "592f09eb-7c25-4f47-efcf-a00cee27e97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nltk_convert = []\n",
        "s = ''\n",
        "for word in english_clean[0]:  # конвертируем в строку\n",
        "  s = s + word + ' '\n",
        "tokenize = nltk.pos_tag(nltk.word_tokenize(s))\n",
        "#print(tokenize)\n",
        "\n",
        "for tok in tokenize:  # заменяем теги на нужные нам\n",
        "\n",
        "  if tok[1] == 'DT':\n",
        "    nltk_convert.append('DET')\n",
        "  elif tok[1] == 'IN':\n",
        "    nltk_convert.append('PREP')\n",
        "  elif tok[1] == 'CC':\n",
        "    nltk_convert.append('CONJ')\n",
        "  elif tok[1] == 'TO':\n",
        "    nltk_convert.append('PART')\n",
        "  elif tok[1] == 'NN' or tok[1] == 'NNS' or tok[1] == 'NNPS' or tok[1] == 'NNP':\n",
        "    nltk_convert.append('NOUN')\n",
        "  elif tok[1] == 'PRP' or tok[1] == 'PRP$':\n",
        "    nltk_convert.append('PRO')\n",
        "  elif tok[1] == 'RB' or tok[1] == 'WRB' or tok[1] == 'WP' or tok[1] == 'WDT':\n",
        "    nltk_convert.append('ADV')\n",
        "  elif tok[1] == 'CD':\n",
        "    nltk_convert.append('NUM')\n",
        "  elif tok[1] == 'UH':\n",
        "    nltk_convert.append('ADJ')\n",
        "  elif tok[1] == 'VB' or tok[1] == 'VBD' or tok[1] == 'VBN' or tok[1] == 'VBP' or tok[1] == 'VBG' or tok[1] == 'VBZ':\n",
        "    nltk_convert.append('VERB')\n",
        "  elif tok[1] == 'JJ' or tok[1] == 'JJR' or tok[1] == 'JJS':\n",
        "    nltk_convert.append('ADJ')\n",
        "  else:\n",
        "    nltk_convert.append(tok[1])\n",
        "    \n",
        "print(nltk_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NOUN', 'NOUN', 'PART', 'DET', 'NOUN', 'NOUN', 'VERB', 'PRO', 'ADJ', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PART', 'VERB', 'ADJ', 'ADV', 'VERB', 'PRO', 'NOUN', 'DET', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'PREP', 'ADJ', 'CONJ', 'PRO', 'VERB', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'VERB', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRO', 'CONJ', 'NUM', 'NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADV', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRO', 'VERB', 'NUM', 'ADJ', 'NOUN', 'PRO', 'PRO', 'VERB', 'ADJ', 'NOUN', 'VERB', 'ADV', 'VERB', 'DET', 'NOUN', 'PREP', 'PRO', 'NOUN', 'DET', 'NOUN', 'VERB', 'ADV', 'ADJ', 'PART', 'VERB', 'CONJ', 'VERB', 'PREP', 'PRO', 'NOUN', 'PRO', 'VERB', 'DET', 'NOUN', 'ADV', 'VERB', 'DET', 'NOUN', 'CONJ', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADV', 'VERB', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHWgWhx10jU1",
        "outputId": "45783038-863b-4658-c858-bef5f1c83d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(english_clean[1], nltk_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6491228070175439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUQHcpgZ5BF2"
      },
      "source": [
        "**FLAIR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjBWciD46bm6",
        "outputId": "308164cb-ff15-4622-8882-91c97d0e6b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load('pos-multi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-18 15:52:52,658 loading file /root/.flair/models/pos-multi-v0.1.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlJMS0W38WK_",
        "outputId": "38e5178d-f6e0-491c-9157-a5a9313d6f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentence = Sentence(s)\n",
        "\n",
        "# predict pos tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "tags_flair = sentence.to_tagged_string()\n",
        "print(tags_flair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i <PRON> goed <VERB> to <ADP> the <DET> shop <NOUN> twitter <NOUN> is <AUX> my <PRON> absolutely <ADV> fav <ADJ> social <ADJ> media <NOUN> facebook <NOUN> is <AUX> the <DET> way <NOUN> to <ADP> baaad <VERB> i <PRON> totally <ADV> love <VERB> your <PRON> twts <NOUN> this <PRON> is <AUX> a <DET> subtweet <NOUN> of <ADP> anyone <PRON> in <ADP> particular <ADJ> and <CCONJ> it <PRON> is <AUX> going <VERB> to <PART> annoy <VERB> some <DET> people <NOUN> i <PRON> have <VERB> a <DET> lot <NOUN> of <ADP> phd <NOUN> followers <NOUN> and <CCONJ> friends <NOUN> and <CCONJ> i <PRON> am <AUX> a <DET> stubborn <ADJ> guy <NOUN> myself <PRON> but <CCONJ> one <NUM> thing <NOUN> i <PRON> am <AUX> noticed <VERB> is <VERB> when <ADV> someone <PRON> gets <VERB> a <DET> phd <NOUN> it <PRON> become <VERB> 10 <NUM> harder <ADJ> convince <VERB> them <PRON> they <PRON> are <AUX> wrong <ADJ> i <PRON> have <AUX> actually <ADV> found <VERB> the <DET> opposite <NOUN> in <ADP> my <PRON> discipline <NOUN> the <DET> phd <NOUN> are <AUX> generally <ADV> open <ADJ> to <ADP> critique <NOUN> and <CCONJ> questioning <NOUN> of <ADP> their <PRON> work <NOUN> it <PRON> is <AUX> the <DET> people <NOUN> who <PRON> have <VERB> an <DET> mba <NOUN> but <CCONJ> no <DET> practical <ADJ> business <NOUN> experience <NOUN> that <PRON> are <AUX> absolute <ADJ> monsters <NOUN>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMUNZjYq95tH",
        "outputId": "579efb0c-2c3b-46f6-988c-44b2d11272eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "flair_convert = []\n",
        "\n",
        "search = re.findall('\\<\\w+\\>', tags_flair)\n",
        "for tag in search:\n",
        "  tag = re.sub('<PROPN>$', 'NOUN', tag)\n",
        "  tag = re.sub('<AUX>$', 'VERB', tag)\n",
        "  tag = re.sub('<ADP>$', 'PREP', tag)\n",
        "  tag = re.sub('<CCONJ>$', 'CONJ', tag)\n",
        "  tag = re.sub('<PRO>$', 'PRON', tag)\n",
        "\n",
        "  flair_convert.append(tag.replace('<','').replace('>',''))\n",
        "\n",
        "print(flair_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PRON', 'VERB', 'PREP', 'DET', 'NOUN', 'NOUN', 'VERB', 'PRON', 'ADV', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PREP', 'VERB', 'PRON', 'ADV', 'VERB', 'PRON', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'PREP', 'PRON', 'PREP', 'ADJ', 'CONJ', 'PRON', 'VERB', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRON', 'CONJ', 'NUM', 'NOUN', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV', 'PRON', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'NUM', 'ADJ', 'VERB', 'PRON', 'PRON', 'VERB', 'ADJ', 'PRON', 'VERB', 'ADV', 'VERB', 'DET', 'NOUN', 'PREP', 'PRON', 'NOUN', 'DET', 'NOUN', 'VERB', 'ADV', 'ADJ', 'PREP', 'NOUN', 'CONJ', 'NOUN', 'PREP', 'PRON', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'CONJ', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PRON', 'VERB', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69kKX5-q_mTs",
        "outputId": "d339b551-48e6-4b94-b214-c024524a2f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(english_clean[1], flair_convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8070175438596491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDdeqhZV_qpR"
      },
      "source": [
        "**Вывод по парсерам**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63EDEWul_t3h"
      },
      "source": [
        "Среди русских парсеров самый лучший результат показал PyMorphy (0.9196428571428571), затем идет Наташа (0.875), хуже всех показал себя Mystem (0.8035714285714286)\n",
        "\n",
        "\n",
        "Среди английских парсеров на первом месте flair (0.8070175438596491), на втором spacy (0.7807017543859649), а хуже всех оказался NLTK (0.6491228070175439)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvHXY_A9A9oB"
      },
      "source": [
        "**ЧАСТЬ 3 - как улучшить код из предыдущего дз?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UOj8lFPB5BX"
      },
      "source": [
        "Для тестирования программы возьмем рандомный хороший и плохой отзыв с кинопоиска "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3fPcMyJDLuL"
      },
      "source": [
        "text = 'не красивый \\\n",
        "сильно мудрый \\\n",
        "не сильно мудрый \\\n",
        "Великолепная картина рассказывает об учителе географии, \\\n",
        "который умеет заворожить непростых подростков и показать им предмет с другой стороны. \\\n",
        "Он устраивает интересные экскурсии и даже берет учеников в поход, каждый из них сталкивается с трудностями и опасностями, \\\n",
        "преодоление которых меняет всех участников на всю оставшуюся жизнь \\\n",
        "На фоне работы, наш учитель так же живет с любимой женой, однако та его больше не любит, \\\n",
        "и наш географ отпускает ее к своему лучшему другу потому, что настоящая любовь выше эгоизма, \\\n",
        "настоящая любовь — это стремление и желание того, чтобы твой объект любви был счастлив.\\\n",
        "Одним из испытаний становится и безответная любовь ученицы и соблазн для нашего взрослого географа, \\\n",
        "которому он не поддается, имея твердый и честный характер. Несмотря на пропащность множества окружающих, \\\n",
        "наш географ остается чист \\\n",
        "очень хороший фильм \\\n",
        "История не милого персонажа живущего в несчастливом браке, находящего отдушину только в собственном деле. \\\n",
        "Качественно выбрано место для съемки, красивая, подвижная локация. \\\n",
        "Актерская игра на уровне, молодые актеры показывают опытный профессионализм, \\\n",
        "а игра Константина Хабенского, как всегда безупречна и многогранна. \\\n",
        "Я нежно и трепетно люблю сказки братьев Гримм и их интерпретации. \\\n",
        "Трейлер на эту версию популярной истории выглядел достаточно красиво и интересно, \\\n",
        "и я с нетерпением ждала выхода фильма — тем более, главную роль исполняет девочка, \\\n",
        "которая прекрасно отыграла Беверли в «Оно». И… Чем дальше в лес  \\\n",
        "Первое, что меня смутило — переделка начала истории, где нет отца и безумной мачехи, \\\n",
        "а мать имеет какое-то расстройство. Ладно, можно простить некоторые отступления от оригинальной истории, \\\n",
        "адаптация на то и адаптация. \\\n",
        "Но дальнейшее повествование слишком уныло и однообразно. \\\n",
        "Моменты, которые должны пугать или навевать на какие-то мысли, \\\n",
        "выполнены слишком прямо и топорно, а визуальные эффекты смотрятся достаточно дешево \\\n",
        "и скорее напоминают отечественные выходки, нежели атмосферу хоррора.\\\n",
        "Можно сделать качественную работу, не выходя за пределы низкого рейтинга, но это конечно случай. \\\n",
        "Героиня никакая, к ней не возникает ни сочувствия, ни сопереживания. \\\n",
        "Эмоциональной подоплеки не хватает, чувство долга перед братом высосано \\\n",
        "как таковая близость между родными не показана, не говоря уже о действительно глупых ляпах \\\n",
        "(в лесу не найти ни кореньев, ни ягод, ни грибов, будучи деревенскими жителями, серьезно?). \\\n",
        "Или же почему героиня не разбирается в растениях, если живет рядом с лесом и должна заботиться о себе и семье? \\\n",
        "Фильм похож на черновик фильма, а не хорошую и качественную работу. ' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5IB541jLiH7",
        "outputId": "282dfac0-ec6f-46d2-8700-bab49aba8468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_list = text.split(' ')\n",
        "len(text_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "379"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zyfWfJwEhPF"
      },
      "source": [
        "1) Прилагательное + существительное (без не перед прилагательным) - очень часто фильмы описываются такой конструкцией - ужасный фильм / хороший фильм. Чтобы избежать путаницы вида хороший фильм - не хороший фильм отберем только прил + сущ без не перед прил \n",
        "\n",
        "2) Наречие + прилагательное - очень хороший / очень плохой, эта конструкция тоже часто используется для описание. Вновь для того, чтобы избежать неоднозначности возьмем только без не перед наречием\n",
        "\n",
        "3) не + глагол - очень часто используется - типа не понравился / не впечатлил - для плохих отзывов, не разочаровал - для хороших \n",
        "\n",
        "4) не + прилагательное - также часто используется. Например, для плохих - не хороший, не впечатляющий; для хороших - не разочаровавший, иногда опечатки типа 'не обыкновенный'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xInztyiJW00"
      },
      "source": [
        "Лучший парсер для русского это пайморфи, поэтому используем его"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoXXjiyIQ1PD"
      },
      "source": [
        "*выведем прилагательное + существительное и при этом 'не' не стоит перед прилагательным*\n",
        "\n",
        "\n",
        "*выведем наречие + прилагательное и при этом 'не' не стоит перед наречием*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdvAnaROEekx",
        "outputId": "278b5fbd-9fce-4f53-f2ed-1859283fd416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tags = []\n",
        "words = []\n",
        "\n",
        "adj_noun = ' '\n",
        "adv_adj = ' '\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "for i in text_list:\n",
        "  p = morph.parse(i)[0]\n",
        "  words.append(i)\n",
        "  tags.append(p.tag.POS)\n",
        "\n",
        "s = 0  # счетчик\n",
        "for tag in tags:\n",
        "  try:\n",
        "    if tag == 'ADJF' and tags[s + 1] == 'NOUN' and words[s - 1] != 'не':  # прилагательное + существительное без не перед прил\n",
        "      adj_noun += str(words[s]).replace(',','') + ' ' + str(words[s + 1]).replace(',','') + '; '\n",
        "      s+=1\n",
        "\n",
        "    if tag == 'ADVB' and tags[s + 1] == 'ADJF'and words[s - 1] != 'не':   # наречие + прилагательное без не перед наречией\n",
        "      adv_adj += str(words[s]).replace(',','') + ' ' + str(words[s + 1]).replace(',','') + '; '\n",
        "      s +=1\n",
        "\n",
        "    else:\n",
        "      s +=1\n",
        "  except:\n",
        "    IndexError\n",
        "\n",
        "print('Прилагательное + существительное:', adj_noun)\n",
        "\n",
        "print('Наречие + прилагательное:', adv_adj)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Прилагательное + существительное:  Великолепная картина; оставшуюся жизнь; находящего отдушину; выбрано место; молодые актеры; а игра; перед братом; \n",
            "Наречие + прилагательное:  сильно мудрый; \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKqafw5RRr-"
      },
      "source": [
        "*выведем не + глагол и не + прилагательное*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArIvVrZ5RRHR",
        "outputId": "bd91e6db-44bd-4a89-dc31-3ea1627c33d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ne_verb = ' '\n",
        "ne_adj = ' '\n",
        "\n",
        "s = 0\n",
        "for tag in tags:\n",
        "  try:\n",
        "\n",
        "    if tag == 'VERB' and words[s - 1] == \"не\":  # не + глагол \n",
        "      ne_verb += str(words[s - 1]) + ' ' + str(words[s]) + '; '\n",
        "    if tag == 'ADJF' and words[s - 1] == \"не\":  # не + прилагательное\n",
        "      ne_adj += str(words[s - 1]) + ' ' + str(words[s]) + '; '\n",
        "      s +=1\n",
        "    else:\n",
        "      s +=1\n",
        "  except:\n",
        "    IndexError\n",
        "\n",
        "print('Не + глагол:', ne_verb)\n",
        "\n",
        "print('Не + прилагательное:', ne_adj)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Не + глагол:  не возникает; не разбирается; \n",
            "Не + прилагательное:  не красивый; не милого; не хорошую; \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}