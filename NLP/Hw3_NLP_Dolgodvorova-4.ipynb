{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw3 NLP Dolgodvorova.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nTFDYPONh_K"
      },
      "source": [
        "**Часть 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS6kpr-vN6KS"
      },
      "source": [
        "**C mallet у меня ошибка, как у всех, поэтому я брала gensim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4HfxACh7lJ"
      },
      "source": [
        "#!pip install pyLDAvis\n",
        "#!pip install nltk\n",
        "#!pip install spacy"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2QsKwBfijuo",
        "outputId": "5f3b28be-2da3-4405-e4b3-03d11e506c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwMiXyIimlt"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4KYJGosiq1Y",
        "outputId": "286eb338-f483-4edf-d40a-6bf992db201a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(df.target_names.unique())\n",
        "df.head()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
            " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
            " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
            " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
            " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
            " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ...           target_names\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...              rec.autos\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...  comp.sys.mac.hardware\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...  comp.sys.mac.hardware\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...          comp.graphics\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...              sci.space\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWkZsYQivYv",
        "outputId": "905ac1cf-819b-4e8b-8d7d-8e890e430e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "print(data[:1])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HnT7Yai0XX",
        "outputId": "caf21b20-eae3-4a09-8822-a9f2f39b1e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#tokenize\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIronX5i3TR",
        "outputId": "fddec8e8-a157-4a12-9f6e-e86cc7cc537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlljL6GoqNuA"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRJkg97HqRZK",
        "outputId": "8a2b6e1c-47cf-4dce-e434-b97de13e4fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "!python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAsAloYW-qqg",
        "outputId": "5c3b6685-3136-4f3e-b9bc-d2731eed7079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXHOZU7FBa0a",
        "outputId": "023aecdd-d6f4-4ae5-a46f-77e4eab1f871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('addition', 1),\n",
              "  ('body', 1),\n",
              "  ('bricklin', 1),\n",
              "  ('bring', 1),\n",
              "  ('call', 1),\n",
              "  ('car', 5),\n",
              "  ('could', 1),\n",
              "  ('day', 1),\n",
              "  ('door', 2),\n",
              "  ('early', 1),\n",
              "  ('engine', 1),\n",
              "  ('enlighten', 1),\n",
              "  ('funky', 1),\n",
              "  ('history', 1),\n",
              "  ('host', 1),\n",
              "  ('info', 1),\n",
              "  ('know', 1),\n",
              "  ('late', 1),\n",
              "  ('lerxst', 1),\n",
              "  ('line', 1),\n",
              "  ('look', 2),\n",
              "  ('mail', 1),\n",
              "  ('make', 1),\n",
              "  ('model', 1),\n",
              "  ('name', 1),\n",
              "  ('neighborhood', 1),\n",
              "  ('nntp_poste', 1),\n",
              "  ('park', 1),\n",
              "  ('production', 1),\n",
              "  ('really', 1),\n",
              "  ('rest', 1),\n",
              "  ('see', 1),\n",
              "  ('separate', 1),\n",
              "  ('small', 1),\n",
              "  ('sport', 1),\n",
              "  ('tellme', 1),\n",
              "  ('thank', 1),\n",
              "  ('thing', 1),\n",
              "  ('where', 1),\n",
              "  ('wonder', 1),\n",
              "  ('year', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9igpVWaNOUc"
      },
      "source": [
        "**Часть 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5grybeABd95"
      },
      "source": [
        "def which_is_better(corpus):  \n",
        "  coh_score = []  # список, где хранятся значения coherence score\n",
        "  n = range(10,16)  # для итерации по числу топиков \n",
        "  num_top = list(n)  # список с числом топиков (нужно для словаря)\n",
        "\n",
        "# обучаем модель\n",
        "  for i in n:\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= int(i), \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "  \n",
        "# находим coherence_score\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    coh_score.append(coherence_lda)\n",
        "\n",
        "# создаем словарь coherence score : число топиков \n",
        "  dictionary = dict(zip(coh_score, num_top))\n",
        "\n",
        "  a = max(coh_score)  # найдем максимум coh_score \n",
        "\n",
        "  best = dictionary[a]  \n",
        "\n",
        "  return best"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdjZK-fNCAaJ"
      },
      "source": [
        "best = which_is_better(corpus)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSz_wO33aFZc"
      },
      "source": [
        "**Часть 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyrNG9KGNAzl",
        "outputId": "f95076bb-a313-4841-ce3b-08614f985e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(int(best))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12qUe7YXZyWy"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= int(best), \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s-7673-bd5W"
      },
      "source": [
        "spisok = lda_model.print_topics()\n",
        "doc_lda = lda_model[corpus]\n"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vb3A3fSAdSg"
      },
      "source": [
        "Разделим слова и веса слов на два списка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWM_9Ky_ijaE"
      },
      "source": [
        "s = 0\n",
        "slov = []  # список списков слов\n",
        "ves = []  # список списков весов\n",
        "for i in spisok:\n",
        "  slova = ''\n",
        "  vesa = ''\n",
        "  a = (spisok[s][1])\n",
        "  s+=1\n",
        "  slova = re.sub(r'[^\\w\\s]+|[\\d]+', r'',a).strip()  # убираем лишнее\n",
        "  vesa = re.sub(r'[A-Za-z\"*+_]', r'',a).strip()  # убираем лишнее\n",
        "  ves.append((vesa).split(' ')[::2])\n",
        "  slov.append((slova).split(' ')[::2])"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6J4pvGZAS2D"
      },
      "source": [
        "Создадим словарь, чтобы иметь доступ к весу по слову"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u_mY5_AkZMQ"
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZAeNZH0keBF",
        "outputId": "360d393c-2689-472c-cb2b-adeb1ca08091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "merged_vesa = list(itertools.chain.from_iterable(ves))  # список весов\n",
        "merged_vesa[::40]"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.028', '0.090', '0.021']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sfIamJjk_cI",
        "outputId": "a50486f3-dfaf-4380-983c-d83c5d933433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "merged_slova = list(itertools.chain.from_iterable(slov))  # список слов\n",
        "merged_slova[::90]"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['team', 'key']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzQFfocCqV_A"
      },
      "source": [
        "d = dict(zip(merged_slova, merged_vesa))\n"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XY61YofqbCQ",
        "outputId": "88a9404a-54c7-4141-8431-baf05f1eece8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sf = 0\n",
        "text_numb = []\n",
        "most_imp = []\n",
        "\n",
        "for lemma in data_lemmatized:\n",
        "  text_numb.append(sf)\n",
        "  sf += 1\n",
        "  # нужно 10 весов, тк лучший coherent score с 10 топиками\n",
        "  null, one, two, three, four, five = 0, 0, 0, 0, 0, 0\n",
        "  six, seven, eight, nine = 0, 0, 0, 0\n",
        "\n",
        "  for l in lemma:\n",
        "    if l in slov[0]:\n",
        "      null += float(d[l])\n",
        "    elif l in slov[1]:\n",
        "      one += float(d[l])\n",
        "    elif l in slov[2]:\n",
        "      two += float(d[l])\n",
        "    elif l in slov[3]:\n",
        "      three += float(d[l])\n",
        "    elif l in slov[4]:\n",
        "      four += float(d[l])\n",
        "    elif l in slov[5]:\n",
        "      five += float(d[l])\n",
        "    elif l in slov[6]:\n",
        "      six += float(d[l])\n",
        "    elif l in slov[7]:\n",
        "      seven += float(d[l])\n",
        "    elif l in slov[8]:\n",
        "      eight += float(d[l])\n",
        "    elif l in slov[9]:\n",
        "      nine += float(d[l])\n",
        "    else:\n",
        "      pass\n",
        "  topics = ['null', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "  vesy = (str(null) + ' ' + str(one) + ' ' + str(two) + ' ' + str(three) + ' ' + str(four) + \n",
        "        ' ' + str(five) + ' ' + str(six) + ' ' + str(seven) + ' ' + str(eight) + ' ' + str(nine)).split(' ')\n",
        "  dic = dict(zip(vesy, topics))\n",
        "  a = str(max (null, one, two, three, four, five, six, seven, eight, nine))\n",
        "  most_imp.append(dic[a])\n",
        "print(most_imp[::100])  #  главный топик для каждого текста"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['three', 'six', 'seven', 'seven', 'six', 'seven', 'six', 'nine', 'six', 'seven', 'one', 'seven', 'one', 'null', 'one', 'seven', 'one', 'six', 'seven', 'seven', 'seven', 'six', 'one', 'nine', 'six', 'eight', 'seven', 'six', 'two', 'seven', 'seven', 'five', 'one', 'three', 'eight', 'one', 'six', 'null', 'seven', 'seven', 'nine', 'six', 'seven', 'seven', 'seven', 'seven', 'three', 'six', 'seven', 'seven', 'seven', 'six', 'eight', 'one', 'seven', 'seven', 'six', 'seven', 'eight', 'seven', 'one', 'six', 'six', 'six', 'seven', 'seven', 'six', 'seven', 'one', 'six', 'seven', 'eight', 'seven', 'six', 'six', 'one', 'seven', 'null', 'seven', 'five', 'seven', 'seven', 'seven', 'six', 'six', 'six', 'nine', 'five', 'null', 'seven', 'seven', 'seven', 'six', 'six', 'seven', 'six', 'seven', 'six', 'seven', 'two', 'seven', 'seven', 'seven', 'three', 'six', 'seven', 'seven', 'one', 'five', 'seven', 'seven', 'seven', 'seven', 'eight']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61iSPoBRqe6J"
      },
      "source": [
        "# объединим тексты с одинаковым главным топиком\n",
        "s = 0\n",
        "sp0, sp1, sp2, sp3, sp4, sp5, sp6, sp7, sp8, sp9 = [], [], [], [], [], [], [], [], [], []\n",
        "for i in most_imp:\n",
        "  if i == 'null':\n",
        "    sp0.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'one':\n",
        "    sp1.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'two':\n",
        "    sp2.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'three':\n",
        "    sp3.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'four':\n",
        "    sp4.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'five':\n",
        "    sp5.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'six':\n",
        "    sp6.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'seven':\n",
        "    sp7.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'eight':\n",
        "    sp8.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  elif i == 'nine':\n",
        "    sp9.append(' '.join(data_lemmatized[s]))\n",
        "    s += 1\n",
        "  else:\n",
        "    s += 1"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhU-eqQJdHor"
      },
      "source": [
        "**Часть 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_EanW3ASERg"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tf_idf_vectorize = TfidfVectorizer(use_idf=True)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfFnKMAPErwo"
      },
      "source": [
        "Получим датафрейм; столбец text - текст, topic_number - номер топика ( всего их у нас 10, те от 0 до 9 включительно), five_words - пять самых важных слов, определенных при помощи TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZhVVFsS-pDw",
        "outputId": "8a454aa1-a3ed-492a-c211-d03bb4ef7f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# функция итерирует по топикам с текстами, выбирая для каждого текста в каждом топика 5 главных слов\n",
        "\n",
        "df = pd.DataFrame(columns=['text', 'topic_number', 'five_words'])  # итоговый датафрейм\n",
        "texts = [sp0, sp1, sp2, sp3, sp4, sp5, sp6, sp7, sp8, sp9]  # тексты по топикам\n",
        "top = 0\n",
        "\n",
        "for t in texts:  # итерация по топикам\n",
        "  s = 0\n",
        "  vectors = tf_idf_vectorize.fit_transform(t)\n",
        "\n",
        "  for i in t:  # итерация по текстам внутри топика\n",
        "    vec = vectors[s]\n",
        "    dfvec = pd.DataFrame(vec.T.todense(), index=tf_idf_vectorize.get_feature_names(), columns=[\"tf_idf\"]) \n",
        "    five_words = list(dfvec.sort_values(by=[\"tf_idf\"],ascending=False).index[:5])\n",
        "    df = df.append({'text': i, 'topic_number': top, 'five_words': ', '.join(five_words)}, ignore_index=True)\n",
        "    s+=1\n",
        "\n",
        "  top+=1\n",
        "\n",
        "df[::100]"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>topic_number</th>\n",
              "      <th>five_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>win icon help please line win download several...</td>\n",
              "      <td>0</td>\n",
              "      <td>icon, help, bmp, wallpaper, appreciated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>obispo line strike else silly impose game susp...</td>\n",
              "      <td>0</td>\n",
              "      <td>suspension, sad, impose, mathematic, whittier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>sad write foul illegal contact player stick bo...</td>\n",
              "      <td>0</td>\n",
              "      <td>foul, penalty, penalty_shot, award, shot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>reply organization line article session write ...</td>\n",
              "      <td>0</td>\n",
              "      <td>israeli, attack, session, target, fair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>medium wing epa host epa utoronto_ca line news...</td>\n",
              "      <td>0</td>\n",
              "      <td>epa, tune, leafs, wish, wing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10900</th>\n",
              "      <td>write canonical book add much late churchs his...</td>\n",
              "      <td>8</td>\n",
              "      <td>whoah, add, book, protestant, much</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11000</th>\n",
              "      <td>window keeps_crashing help line nntp_poste hos...</td>\n",
              "      <td>9</td>\n",
              "      <td>stack, crash, keeps_crashing, set, apr_gmt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11100</th>\n",
              "      <td>help change line write crosspost vga card corr...</td>\n",
              "      <td>9</td>\n",
              "      <td>specially, file, bmp, compress, screen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11200</th>\n",
              "      <td>net security xxdate_fri line call paper intern...</td>\n",
              "      <td>9</td>\n",
              "      <td>submission, symposium, chair, panel, security</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11300</th>\n",
              "      <td>line description general principle clipper pro...</td>\n",
              "      <td>9</td>\n",
              "      <td>chunk, bit, key, system, fixable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...                                     five_words\n",
              "0      win icon help please line win download several...  ...        icon, help, bmp, wallpaper, appreciated\n",
              "100    obispo line strike else silly impose game susp...  ...  suspension, sad, impose, mathematic, whittier\n",
              "200    sad write foul illegal contact player stick bo...  ...       foul, penalty, penalty_shot, award, shot\n",
              "300    reply organization line article session write ...  ...         israeli, attack, session, target, fair\n",
              "400    medium wing epa host epa utoronto_ca line news...  ...                   epa, tune, leafs, wish, wing\n",
              "...                                                  ...  ...                                            ...\n",
              "10900  write canonical book add much late churchs his...  ...             whoah, add, book, protestant, much\n",
              "11000  window keeps_crashing help line nntp_poste hos...  ...     stack, crash, keeps_crashing, set, apr_gmt\n",
              "11100  help change line write crosspost vga card corr...  ...         specially, file, bmp, compress, screen\n",
              "11200  net security xxdate_fri line call paper intern...  ...  submission, symposium, chair, panel, security\n",
              "11300  line description general principle clipper pro...  ...               chunk, bit, key, system, fixable\n",
              "\n",
              "[114 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASNUBD9qElup"
      },
      "source": [
        "**Часть 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UilS1_whEnow"
      },
      "source": [
        "**Подсчет Coherence score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbsH7t0H7fy"
      },
      "source": [
        "1 этап - набор слов t (то есть, по сути, набор данных) делится на набор пар подмножеств слов S\n",
        "\n",
        "2 этап - вероятности слов P вычисляются на основе заданного эталонного корпуса\n",
        "\n",
        "3 этап - S и P нужны для подсчета согласованности между парами подмножеств S\n",
        "\n",
        "4 этап - объединение значений до одного значения когерентности c\n",
        "\n",
        "Для Topic coherence используется две метрики для подсчета согласованности темы - внутренняя и внешняя\n",
        "\n",
        "Внутренняя - представлена как Umass. Проводит измерения, чтобы сравнить слово только с предыдущими и последующими словами соответственно, поэтому нужен упорядоченный набор слов.  Используется как функция попарной оценки в виде эмпирической условной логарифмической вероятности со сглаживающим подсчетом, чтобы избежать вычисления логарифма нуля\n",
        "\n",
        "Внешняя - представлена как UCI. В измерении UCI каждое отдельное слово сопряжено с каждым другим отдельным словом. UCIcoherence использует PMI. PMI - рассчитывается как log  от деления числа документов (содержащих оба слова) на произведения количества документов (содержащих слово 1) и количества документов (содержащих слово 2)\n",
        "\n",
        "\n",
        "Как внутренняя, так и внешняя мера вычисляют балл когерентности c (сумма попарных баллов по словам w1,..., wn, используемым для описания темы)"
      ]
    }
  ]
}